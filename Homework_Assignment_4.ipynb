{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please **submit this Jupyter notebook through Canvas** no later than **Monday November 19, 12:59**, before the start of the lecture.\n",
    "\n",
    "Homework is in **groups of two**, and you are expected to hand in original work. Work that is copied from another group will not be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0\n",
    "Write down the names + student ID of the people in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jordan Earle** (12297127)\n",
    "\n",
    "**Natasja Wezel** (11027649)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import diags\n",
    "import scipy.linalg\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "# Exercise 1\n",
    "# (a)\n",
    "Let $Q$ be a real, square orthogonal matrix. Show that all eigenvalues of $Q$ satisfy $|\\lambda |=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda$ is an eigenvalue of Q with corresponding eigenvector $x$, $Qx = \\lambda x$. This would mean that $||Qx||_2 = ||\\lambda x||_2$.  An orthoginal matrix such as Q, operating an vector x, would only cause a rotation, reflection or retroreflection, and since an orthogonal matrix preserves the 2-norm, we have $||x||_2 = \\lambda \\cdot ||x||_2$, which would mean that $|\\lambda| = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    Qx &= \\lambda x\\\\\n",
    "    ||Qx||_2 &= ||\\lambda x||_2\\\\\n",
    "    ||x||_2 &= \\lambda \\cdot ||x||_2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# (b)\n",
    "A projection matrix is a matrix $P$ for which $P^2=P$. Find the eigenvalues of a projection matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda$ is an eigenvalue of P with corresponding eigenvector $x$, $Px = \\lambda x$. This would mean that $P^2x = \\lambda Px = \\lambda^2 x$. This means that $\\lambda^2$ is an eigenvalue of $P^2$. If $P = P^2$, the eigenvalues are thus such that $\\lambda^2 = \\lambda$. The only possible values which satisfy this are 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    Px &= \\lambda x\\\\\n",
    "    PPx &= P \\lambda x\\\\\n",
    "    P^2x &= \\lambda Px\\\\\n",
    "    P^2 x &= \\lambda^2 x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Exercise 2\n",
    "Generate a symmetric $4 × 4$ matrix $A$ randomly, e.g. as below.\n",
    "Implement Rayleigh quotient iteration (Algorithm 4.4) to compute the largest eigenvalue and corresponding eigenvector of $A$. Use a suitable stopping criterion for your iterations (and explain why you think it is suitable). You may use `scipy.linalg.solve`.\n",
    "\n",
    "If $x$ is an approximate eigenvector for a real matrix $A$, then determining the best estimate for the corresponding eigenvalue $\\lambda$ can be considered as an $n \\times 1$ linear least squares approximate problem: $x \\lambda \\cong Ax$. The matrix has a normal equation $x^Tx\\lambda = x^TAx$ and from this we can see that the least squares solution is given by:\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{x^TAx}{x^Tx}\n",
    "$$\n",
    "\n",
    "This quantity is known as the *Rayleigh quotient* and can be used to accelerate the convergence of a method such as power iteration, since at iteration *k* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the matrix: \n",
      " [[-3.44610212  3.39605219  2.43548198  2.09382226]\n",
      " [ 3.39605219  1.08165391  0.89106994 -0.86282963]\n",
      " [ 2.43548198  0.89106994  1.51756459  0.07548154]\n",
      " [ 2.09382226 -0.86282963  0.07548154  2.18030895]]\n",
      "The dominant Eigenvalue is approx:  [[-6.26604945]]\n",
      "The dominant Eigenvector is approx: \n",
      " [[-0.85481402]\n",
      " [ 0.39789707]\n",
      " [ 0.21948862]\n",
      " [ 0.25059062]]\n",
      "The true dominant eigenvalue is \n",
      " [-6.26605121]\n",
      "The true dominant eigenvector is \n",
      " [[-0.85481402  0.49735283  0.12873873  0.07320857]]\n"
     ]
    }
   ],
   "source": [
    "B = np.random.randn(4,4)\n",
    "A = B+B.T\n",
    "#A = np.array([[3.,1.], [1., 3.]])\n",
    "I = np.eye(A.shape[0])\n",
    "# Need to add in a check to make sure the matrix is ok, sometimes it crashes\n",
    "#print(I)\n",
    "#print(A)\n",
    "\n",
    "x = np.random.rand(A.shape[0],1)\n",
    "\n",
    "escbreak = True\n",
    "\n",
    "prev_sigma = 0\n",
    "\n",
    "for potatto in range(0, 10):\n",
    "    x = A.dot(x)\n",
    "\n",
    "while escbreak:\n",
    "    \n",
    "    xT = x.T\n",
    "    # compute shift\n",
    "    sigma = xT.dot(A.dot(x))/(xT.dot(x))\n",
    "    left = (A - sigma*I)\n",
    "    \n",
    "    # generate next vector\n",
    "    ycur = np.linalg.solve(left, x)\n",
    "    \n",
    "    # normalize the vector\n",
    "    x = ycur/np.linalg.norm(ycur, ord = 2)\n",
    "\n",
    "    # check to see if the loop should break \n",
    "    \n",
    "    if (np.linalg.norm(A.dot(x), ord = 2) - np.linalg.norm(sigma*x, ord = 2)) < 0.0001:\n",
    "        escbreak = False\n",
    "    prev_sigma = sigma\n",
    "\n",
    "# find actual eigenvalues    \n",
    "w, v = sp.linalg.eig(A)\n",
    "wabs = abs(w.real)\n",
    "max_eig = max(wabs)\n",
    "loc = np.where(wabs == max_eig)\n",
    "max_vex = v[loc]\n",
    "max_val = w[loc].real\n",
    "\n",
    "# print the matrix, eigenvalue and eigenvector\n",
    "print(\"For the matrix: \\n\", A)\n",
    "print(\"The dominant Eigenvalue is approx: \", sigma)\n",
    "print(\"The dominant Eigenvector is approx: \\n\", x)\n",
    "print('The true dominant eigenvalue is \\n', max_val)\n",
    "print(\"The true dominant eigenvector is \\n\", max_vex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine a stopping point, a number of methods were discussed. The first and most obvious is that if the value of sigma is not changing much (i.e. less that 0.0001).  This would indicated convergence and was the method used.  Another stopping point would be to stop the system when the determinant of $A-\\sigma I$ becomes 0 as that would indicate that it is singular and that we have found the eigenvalue.  This could be dangerous though if the system does not converge to the exact eigenvalue quickly.  One more method discussed was checking to see if $||Ax||_2 - ||\\lambda x||_2 < certainty value$.  This was implemented as it was more acccurate than trying checking if the value of lambada was not changing quickly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "# Exercise 3\n",
    "Let $A$ be a $n × n$ tridiagonal matrix that results (up to an overall constant scaling factor) from an\n",
    "$n$-point spatial discretization of the Laplace operator in $1$ spatial dimension. Specifically, all diagonal\n",
    "elements of $A$ have the value $-2$, all elements just below or above the diagonal have value $1$.\n",
    "\n",
    "$${\\displaystyle A = \\left( \\begin{array}{rrrrr} -2 &  1  &  0 &  0& \\ldots \\\\ 1  & -2 & 1 & 0 &\\ldots   \\\\ 0 & 1 & -2 &  1 &\\ldots \\\\ \\vdots & & & & \\ddots \\end{array} \\right) \\,}$$\n",
    "\n",
    "\n",
    "\n",
    "Take $n = 150$. Use `scipy.linalg.qr` to implement Algorithm 4.7 from the book and compute the full eigenvalue spectrum of $A$. Compare against\n",
    "the eigenvalues obtained by the built-in routine `scipy.linalg.eig(A)`. Show in a figure the (approximate)\n",
    "eigenvalues after 10, 100 and 500 QR iterations, as well as the eigenvalues obtained with `eig(A)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 150\n",
    "A = diags([1, -2, 1], [-1, 0, 1], shape=(n, n)).toarray()\n",
    "w, v = sp.linalg.eig(A)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.title(\"True Eigenvalues\")\n",
    "plt.hist(w, bins=50)\n",
    "\n",
    "iters = [10,100,150,300,500, 1000, 2000]\n",
    "\n",
    "# cycle through the iterations\n",
    "for i in iters:\n",
    "    # reset the matrix A at the start of each iteration\n",
    "    plt.figure(i)\n",
    "    \n",
    "    for k in range(0,i):\n",
    "        Q, R = sp.linalg.qr(A)\n",
    "        A = R.dot(Q)\n",
    "    \n",
    "    # find the eigenvalues\n",
    "    calceig = []\n",
    "    \n",
    "    for j in range(0, A.shape[0]):\n",
    "        calceig.append(A[j,j])\n",
    "    \n",
    "    #plt.hist(calceig, bins=50)\n",
    "    #plt.title(\"Eigenvalues with \" + str(i) + \" iterations\")\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15,5))\n",
    "    ax1.set_title(\"True eigenvalues\")\n",
    "    ax2.set_title(\"Eigenvalues with \" + str(i) + \" iterations\")\n",
    "    ax1.hist(w, bins=50)\n",
    "    ax2.hist(calceig,bins=50)\n",
    "\n",
    "# we can ignore the warning about casting discards the imaginary part because the imaginary part is 0 anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 150\n",
    "A0 = diags([1, -2, 1], [-1, 0, 1], shape=(n, n)).toarray()\n",
    "w, v = sp.linalg.eig(A0)\n",
    "\n",
    "w = np.sort(w.real)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(range(0,150), w, label=\"True eigenvalues\")\n",
    "plt.title(\"First 150 Eigenvalues of the Matrix\")\n",
    "plt.xlabel('Eigenvalue Number')\n",
    "plt.ylabel('Value of Eignenvalue')\n",
    "\n",
    "iters = [10,100,150,300,500, 1000, 2000]\n",
    "eigniters = []\n",
    "# cycle through the iterations\n",
    "for i in iters:\n",
    "    # reset the matrix A at the start of each iteration\n",
    "    A = A0\n",
    "    \n",
    "    for k in range(0,i):\n",
    "        Q, R = sp.linalg.qr(A)\n",
    "        A = R.dot(Q)\n",
    "    \n",
    "    # find the eigenvalues\n",
    "    calceig = []\n",
    "    \n",
    "    for j in range(0, A.shape[0]):\n",
    "        calceig.append(A[j,j])\n",
    "    eigniters.append(calceig)\n",
    "    plt.plot(range(0,150), calceig, label=\"Eigenvalues with \" + str(i) + \" iterations\", alpha=.40)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Absolute error in calculated eigenvalues\")\n",
    "for i in range(len(iters)):\n",
    "    value = abs(w- eigniters[i])\n",
    "    plt.plot(value, label=\"Eigenvalues with \" + str(iters[i]) + \" iterations\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
